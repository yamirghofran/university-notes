---
title: Unsupervised Learning
---

Use unlabeled data, modeling the underlying structure or distribution.

## Main Algorithms
- [K-Means Clustering](/machine-learning-foundations/k-means-clustering)
- [Density-Based Spatial Clustering of Applications with Noise](/machine-learning-foundations/density-based-spatial-clustering-of-applications-with-noise) (DBSCAN)
- [Gaussian Mixture Models](/machine-learning-foundations/gaussian-mixture-models) (GMM)
- [Hierarchical Clustering](/machine-learning-foundations/hierarchical-clustering)
- [Principal Component Analysis 1](/machine-learning-foundations/principal-component-analysis-1) (PCA)
- [t-SNE](/machine-learning-foundations/t-sne)
- [Spectral Clustering](/machine-learning-foundations/spectral-clustering)
- [Isomap](/machine-learning-foundations/isomap)
- [AutoEncoder](/machine-learning-foundations/autoencoder)

## Examples
### [Clustering](/machine-learning-foundations/clustering)
Grouping similar data points based on intrinsic characteristics. e.g. identifying natural groupings or sub-populations in data; highlighting patterns or anomalies (e.g. segmentation in marketing)

### Dimensionality Reduction
Reducing the number of features or dimensions in a dataset while retaining as much relevant information as possible. e.g. noise reduction, visualization, computational complexity.

### Outlier Detection
Identifying data points that deviate significantly from the majority of the dataset. These outliers may represent rare or anomalous events, errors, or unusual patterns.
