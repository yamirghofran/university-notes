---
title: Topics List
---

# Chapter 1
## Define, describe, and relate
- [Machine learning](/machine-learning-foundations/machine-learning)
- [example](/machine-learning-foundations/notation)
- [feature vector](/machine-learning-foundations/feature-vector)
- [Feature](/machine-learning-foundations/feature-vector)
- [Label](/machine-learning-foundations/feature-vector)
- [[Y2Q2/Machine Learning Foundations/Supervised Learning|Supervised Learning]]
- [[Y2Q2/Machine Learning Foundations/Unsupervised Learning|Unsupervised Learning]]
- [[Y2Q2/Machine Learning Foundations/Reinforcement Learning|Reinforcement Learning]]
- #ml/classification classification
- #ml/regression regression
## Questions
- What are the pros and cons of supervised, unsupervised, reinforcement learning?
- When should machine learning be [used](/machine-learning-foundations/when-to-not-use-ml)?
	- Problem too complex to code
	- Problem constantly changing
	- Perceptive problem
	- Unstudied phenomenon
	- Simple objective
	- cost-effective
- When should machine learning [not be used](/machine-learning-foundations/when-to-not-use-ml)?
	- Every decision needs to be explainable
	- Cost of an error is too high
	- Heuristic is good enough
	- We want to go to market as fast as possible
	- Can't get right data
	- Build a system the won't be improved over time
- What is the main steps in a [ML project lifecycle](/machine-learning-foundations/machine-learning-engineering)?
	- Goal definition
	- Dataset preparation
	- Feature Engineering
	- Model training
	- Model evaluation
	- Deployment
	- Serving
	- Monitoring
	- Maintenance
- What is the difference between ML and [ML engineering](/machine-learning-foundations/machine-learning-engineering)?
- Why data are fundamental for [Machine Learning](/machine-learning-foundations/machine-learning)?
- [Validation vs Testing datasets](/machine-learning-foundations/training-and-holdout-datasets)?
# Chapter 2
## Define, describe, and relate
- [complexity](/machine-learning-foundations/estimating-complexity)
- [predictability and reproducibility](/machine-learning-foundations/properties-of-a-successful-model) of a model
- [ML Goal](/machine-learning-foundations/defining-the-goal)
- [model input, output, and success criteria](/machine-learning-foundations/properties-of-a-successful-model)
## Questions
- Examples of [ML impact](/machine-learning-foundations/ml-impact) on existing software systems?
	- Simplifies them by replacing them or improves their performance.
- Three main categories of [ML cost](/machine-learning-foundations/ml-cost)?
	- Complexity of problem
	- Cost of data
	- Needed accuracy (grows superlinearly)
- Can we model the [Complexity](/machine-learning-foundations/estimating-complexity) of an ML solution?
	- no
- What are the two main strategies to [simplify an ML problem](/machine-learning-foundations/estimating-complexity)?
	- Divide and conquer. Simplify
- [Why](/machine-learning-foundations/defining-the-goal) do we build ML models?
- Name and describe [purposes](/machine-learning-foundations/defining-the-goal) of ML models?
- Example of [right way and wrong way](/machine-learning-foundations/when-to-not-use-ml) to use ML models.
- Main reasons [ML projects fail](/machine-learning-foundations/why-ml-projects-fail)?
	- Lack of talent
	- lack of leadership support
	- lack of allignment
	- lack of collaboration
	- lack of infrastructure
	- Data labeling challenges
	- Technically infeasible problem
# Chapter 3
## Define, describe, and relate
- [Good Data](/machine-learning-foundations/good-data)
- [Tomek Links](/machine-learning-foundations/tomek-links)
- [Oversampling](/machine-learning-foundations/oversampling) / [Undersampling](/machine-learning-foundations/undersampling)
- [[SMOTE]] and [ADASYN](/machine-learning-foundations/adasyn)
- [Data Sampling](/machine-learning-foundations/data-sampling)
- [Feature Engineering](/machine-learning-foundations/feature-engineering)
- Encoding
- [Data Augmentation](/machine-learning-foundations/data-augmentation)
## Questions
- What are the three main ways to deal with [data imbalance](/machine-learning-foundations/imbalanced-data)?
	- oversampling (SMOTE, ADASYN), undersampling (random, tomek links)
- Can you describe three types of [data sampling](/machine-learning-foundations/data-sampling)?
	- Simple Random sampling
	- Systematic sampling
	- Stratified sampling
- What is the difference between [One-Hot Encoding](/machine-learning-foundations/one-hot-encoding) and [[Y2Q2/Machine Learning Foundations/Bag of Words|Bag of Words]] encoding?
- What are the [encoding methods](/machine-learning-foundations/feature-engineering) for categorical and numerical data?
	- Onehot encoding
	- Label encoding
	- Mean encoding
	- feature hashing
	- topic modelling
- Why do we want to use n-grams (bag of n-grams)
	- To keep order of tokens into account
- What are the main methods of scikit-learn CountVectorizer?

# Chapter 4
## Define, describe, and relate
- [Mean Encoding](/machine-learning-foundations/mean-encoding)
- [Feature Hashing](/machine-learning-foundations/feature-hashing)
- [Topic Modelling](/machine-learning-foundations/topic-modelling)
- Cutting the long tail
- [Boruta](/machine-learning-foundations/boruta)
- [L1-Regularization (Lasso Regression)](/machine-learning-foundations/l1-regularization-lasso-regression)
- [Clustering](/machine-learning-foundations/clustering)
- [K-Means](/machine-learning-foundations/k-means-clustering)
## Questions
- What are the five main [properties of a good feature](/machine-learning-foundations/properties-of-good-features)?
	- Predictive
	- Easy to compute
	- Uncorrelated
	- Easy to understand & unitary
	- Reliability
- What class of problem fits [Mean encoding](/machine-learning-foundations/mean-encoding), [feature hashing](/machine-learning-foundations/feature-hashing), and [Topic Modelling](/machine-learning-foundations/topic-modelling)?
- Why is [Feature Selection](/machine-learning-foundations/feature-selection) essential?
	- Avoid the curse of dimensionality and sparsity, save computation, more explainable model. Avoid overfitting. Avoid distance metric degredation.
- Which of the following techniques is a heuristic?
- What are the two most common type of feature scaling?
	- Standardization
	- Normalization
- When and how should normalization and standardization be used?
	- Normalization with uniform data
	- Standardization with normal data.
- How do you avoid data leakage in software engineering?
	- Doing manipulation after splitting data separately for each set based on parameters from the training set.
- List three feature engineering best practices.
	- Start simple
	- Cut the long tail
# Chapter 5
## Define, describe, and relate
- [Baseline](/machine-learning-foundations/baseline)
	- Zero rule
- Pipeline
- [Confusion matrix](/machine-learning-foundations/confusion-matrix)
	- TP,FN,FP,TN
- [Precision](/machine-learning-foundations/precision)
- [Recall](/machine-learning-foundations/recall)
- [F-score](/machine-learning-foundations/f-score)/F-measure
- [Accuracy](/machine-learning-foundations/accuracy)
	- cost-sensitive accuracy
	- per-class accuracy
- [Cohen's Kappa Statistic](/machine-learning-foundations/cohens-kappa-statistic)
- [ROC](/machine-learning-foundations/receiver-operating-characteristic)
- [AUC-ROC](/machine-learning-foundations/auc-roc)
- [Discounted Cumulative Gain](/machine-learning-foundations/discounted-cumulative-gain)
- [Normalized Discounted cumulative Gain](/machine-learning-foundations/normalized-discounted-cumulative-gain)
- [Logistic Regression](/machine-learning-foundations/logistic-regression)
- [Random Forest](/machine-learning-foundations/random-forest)
- Ensemble Methods (Gradient Boosting Classifier)
- [Hypterparameter Tuning](/machine-learning-foundations/hypterparameter-tuning)
- [Grid Search](/machine-learning-foundations/grid-search)
- Randomized Search
- Bayesian Optimization
- L1 and L2 [regularization](/machine-learning-foundations/regularization)
- Parameter C
## Questions
- What are the preconditions to [supervised learning](/machine-learning-foundations/supervised-learning)?  
- Mention at least four parameters to consider when selecting a ML algorithm?  
- How do you spotcheck an algorithm?  
- What are the three main [metrics](/machine-learning-foundations/model-performance-metrics) for measuring the supervised regression algorithms   performance?  
- Why does a [confusion matrix](/machine-learning-foundations/confusion-matrix) matter?  
- Why does precision/recall [tradeoff](/machine-learning-foundations/precision-recall-tradeoff) matter?  
- Give at least two examples of how precision/recall [tradeoff](/machine-learning-foundations/precision-recall-tradeoff) applies to a ML problem.  
- How do you tune the precision/recall [tradeoff](/machine-learning-foundations/precision-recall-tradeoff)? 
- Describe the top four metrics for ML classification #ml/classification  algorithms.
- In what conditions should [Accuracy](/machine-learning-foundations/accuracy) be used?
- What is the difference between accuracy, Cohen's k and [ROC](/machine-learning-foundations/receiver-operating-characteristic)?
- What are the main differences between classification and ranking algorithms?
- Why do we normalize the discounted cumulative gain (nDCG)?
- Why do we tune [Hyperparameter](/machine-learning-foundations/parameters-and-hyperparameters)? ([Hypterparameter Tuning](/machine-learning-foundations/hypterparameter-tuning))
- Could we tune them by hand?
- If possible, would it be a good idea?
# Leakage Deep Dive
## Define, describe, and relate
- Data leakage
- Leaky feature
- Group leakage
- Data augmentation
- Temporal leakage
- Data dependence
- Missing value imputation
- Scaling
- Encoding
- Feature Selection
- Dimensionality reduction
- Outlier removal
- Feature augmentation
- Log-power Transform
- Binning
- Feature interactions
## Questions
- Why does data leakage matter?
- When does data leakage happen?
- What is an example of direct data leakage? One of indirect?
- Why do records of the same group need to be kept within the same split set?
- What are two examples of data augmentation? Why can they be leaky?
- What are two of the main difficulties of identifying a temporal data leakage?
- What five feature engineering methods must be applied after splitting the dataset?
- Why should we not refit an encoder or ML model to the validation or testing set?

# Neural Networks
## Define, describe, and relate
- Neuron
- Weight
- Activation Function
- [Loss Function](/machine-learning-foundations/loss-function)
- [Backpropagation](/machine-learning-foundations/backpropagation)
- FFW - [Feedforward Neural Networks](/machine-learning-foundations/feedforward-neural-networks)
- Weighted sum
- Batch Size
- [RNN](/machine-learning-foundations/recurrent-neural-networks)
- [LSTM](/machine-learning-foundations/long-short-term-memory)
- Hidden state
- Vanishing/exploding gradient
- Softmax
- Sigmoid
- tanh
- ReLU
- Word Embeddings
- Encoder
- Decoder
- Self-attention
- multi-head
- embedding
- Multi-class classification
- Multi-label classification
- [AUC-ROC](/machine-learning-foundations/auc-roc)
- [F-Score](/machine-learning-foundations/f-score)
- [Mean Squared Error](/machine-learning-foundations/mean-squared-error) / RMSE
- [Cross-Entropy](/machine-learning-foundations/cross-entropy-loss)
	- CCE
	- BCE
- [Cost Function](/machine-learning-foundations/loss-function)
- Xavier
- He
- Learning Rate
- [Gradient Descent](/machine-learning-foundations/gradient-descent)
- Epochs
- Convergence
- Adam
- Dropout
- Early stopping
- Batch Normalization
## Questions
- What are tho four requirements for [Deep Learning](/machine-learning-foundations/deep-learning)?
- Aside from Neural Networks, what are tho other Deep Learning models/algorithms?
- Why do Neural Networks dominate Machine Learning?
- How does a Neural Network "learn"?
- Formally, what is a Neural Network?
- What is a Neural Network architecture?
- Name four Neural Network architectures.
- What is a dense network?
- Why can FFW and NN in general benefit from GPU/TPUs?
- What kind of encoder should we use for FFW?
- Name five hyperparameters for FFW
- Why do we need a weight initialization method?
- How can we reuse the weights of an already trained NN?
- Why do we need an activation function?
- Why do we need to define a batch size?
- Why RNNs are inherently inefficient?
- Can you define three architectural patterns of an RNN?
- Why are RNNs sequential neural networks?
- In RNNs, does the input order matter? If it does, why?
- Can we use a BoW with an RNN? why/why not?
- Is the hidden state a hyperparameter?
- Is the hidden state size the same as the RNN input size?
- What problem do LSTMs address?
- How are the three gates of a LSTM named?
- Why do we need an output gate in a LSTM?
- What is the difference between Transformers and RNNs?
- How does self-attention work?
- What is the main difference between training and inference in a transformer?
- Why does an encoder need positional encoding?
- What is the main form of parallelism in a transformer?
- Under what conditions is it preferable to use pretrained models?
- What are the tree main strategies to use pretrained models?
- What do I need to finetune a pretrained model?
- For what type of problem can we use pretrained models?
- What do P, C, W, A, and T stand for when training a model?
- What are the common performance metrics for deep neural networks?
- What are the common cost functions for deep Neural Networks?
- What is the relationship between cost and activation functions when training FFWs for multi-class classification?
- What is the main difference between multi-class and multi-label classification?
- What is the difference between One-Hot and Multi-Hot encoders?
- What are the cost function and the activation functions used for multi-class classification? how about multi-label classification?
- Name three parameter initialization methods.
- What are the two distinctive characteristics of He?
- What is the main difference between Xavier and He?
- Why do we need an initialization strategy for a Neural Network?
- Why is learning rate important?
- Why do we need an optimization algorithm for a Neural Network?
- What is the distinctive characteristic of Adam?
- Why does Regularization matter?
- Describe the main stages of a Neural Network training process?
- Name three highly sensitive hyperparameters for Neural Network.

# Chapter 6
## Define, describe, relate
- Transfer Learning
- Meta Model
## Questions
- Main differences between using a shallow or deep learning algorithm and having multimodal inputs and/or outputs?
- What are the two main ways to use transfer learning?
- How do we use a pretrained model to extract features?
- How do we stack deep models?
- What is the main benefit of stacking deep models?
- When stacking models, what is the input of the output layer?