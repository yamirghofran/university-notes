---
title: Unsupervised Learning
---

Use unlabeled data, modeling the underlying structure or distribution.

## Main Algorithms
- [K-Means Clustering](/machine-learning-foundations/algorithms/k-means-clustering)
- [Density-Based Spatial Clustering of Applications with Noise](/machine-learning-foundations/algorithms/density-based-spatial-clustering-of-applications-with-noise) (DBSCAN)
- [Gaussian Mixture Models](/machine-learning-foundations/algorithms/gaussian-mixture-models) (GMM)
- [Hierarchical Clustering](/machine-learning-foundations/algorithms/hierarchical-clustering)
- [[Principal Component Analysis 1]] (PCA)
- [t-SNE](/machine-learning-foundations/feature-engineering/t-sne)
- [Spectral Clustering](/machine-learning-foundations/algorithms/spectral-clustering)
- [Isomap](/machine-learning-foundations/algorithms/isomap)
- [AutoEncoder](/machine-learning-foundations/algorithms/autoencoder)

## Examples
### [Clustering](/machine-learning-foundations/algorithms/clustering)
Grouping similar data points based on intrinsic characteristics. e.g. identifying natural groupings or sub-populations in data; highlighting patterns or anomalies (e.g. segmentation in marketing)

### Dimensionality Reduction
Reducing the number of features or dimensions in a dataset while retaining as much relevant information as possible. e.g. noise reduction, visualization, computational complexity. [Principal Component Analysis](/machine-learning-foundations/algorithms/principal-component-analysis)

### Outlier Detection
Identifying data points that deviate significantly from the majority of the dataset. These outliers may represent rare or anomalous events, errors, or unusual patterns.